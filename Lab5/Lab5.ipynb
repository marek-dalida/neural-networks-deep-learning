{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXPUlLg7PHAF"
   },
   "source": [
    "Neural Networks and Deep Learning\n",
    "Cracow University of Technology\n",
    "\n",
    "Lab Assignment 5:\n",
    "\n",
    "The purpose of this laboratory is to implement a neural network for a classification task:\n",
    "\n",
    "\n",
    "\n",
    "1.   The network is trained using minibatch stochastic gradient descent.\n",
    "2.   You have images of handwritten digits from the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) and you should train the network to predict the value of the digit for images.\n",
    "\n",
    "Network specification:\n",
    "\n",
    "1.   Input layer - one hidden layer - output layer\n",
    "2.   Activation functions: for hidden layer \"ReLU\" and for output layer \"softmax\"\n",
    "3.   Loss function: categorical cross-entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "s8Mq_ZnWWVzU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def der_sigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "\n",
    "def der_relu(x):\n",
    "    return 1. * (x > 0)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    b = x.max()\n",
    "    y = np.exp(x - b)\n",
    "    return y / y.sum()\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y, yHat):\n",
    "    return -np.sum(y * np.log(yHat))\n",
    "\n",
    "\n",
    "def mean_squared_error(y, yHat):\n",
    "    return np.square(np.subtract(y, yHat)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7FFYLpnf6oT"
   },
   "source": [
    "Your code consists of at least five functions:\n",
    "\n",
    "* Network initialization\n",
    "* Forward pass\n",
    "* Backward pass\n",
    "* Train \n",
    "* Evaluate\n",
    "\n",
    "You are free to add more functions for the sake of having better organization for your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7VByNZMXsNa"
   },
   "source": [
    "Tune your network by changing hyperparametes of the network:\n",
    "* Number of epochs\n",
    "* Number of neurons in hidden layer\n",
    "* Different learning rates\n",
    "* Different minibatch sizes\n",
    "\n",
    "Also, try the following changes to the network:\n",
    "* Apply different optimziation algorithms: Momentum, Adagrad, RMSprop, and ADAM\n",
    "* Apply L2 regularization techniques to the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcIVzaTuWT5H"
   },
   "source": [
    "Please submit your code with report on the error rate. You can also compare your results with the MNIST performance results exists on the MNIST website.\n",
    "Please also report the effect of different changes you made in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(x_train[0]) * len(x_train[0][0])\n",
    "hidden_layer_nodes = 32\n",
    "output_size = 10\n",
    "\n",
    "weights = [\n",
    "    np.random.randn(hidden_layer_nodes, input_size),\n",
    "    np.random.randn(output_size, hidden_layer_nodes)\n",
    "]\n",
    "\n",
    "biases = [np.zeros(hidden_layer_nodes), np.zeros(output_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(x, output_num):\n",
    "    result = np.zeros(output_num)\n",
    "    result[x] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_activation(x, weights):\n",
    "    activations = []\n",
    "    activation = x.flatten()\n",
    "    for idx, w in enumerate(weights):\n",
    "        z = np.dot(w, activation) + biases[idx]\n",
    "        if idx < len(weights) - 1:\n",
    "            activation = sigmoid(z)\n",
    "        else:\n",
    "            activation = softmax(z)\n",
    "        activations.append(activation)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_sample(x, y, l1=0.01, l2=0.02):\n",
    "    activations_arr = calc_activation(x, weights)\n",
    "    activation = activations_arr[-1]\n",
    "    one_hot_y = one_hot_encoder(y, output_size)\n",
    "    loss = cross_entropy_loss(one_hot_y, activation)\n",
    "    # apply penalties\n",
    "    for w, b in zip(weights, biases):\n",
    "        loss += l1 * (np.sum(abs(w)) + np.sum(abs(b))) + l2 * (np.sum((w) ** 2) + np.sum((b) ** 2))\n",
    "\n",
    "    one_hot_prediction = np.zeros_like(activation)\n",
    "    one_hot_prediction[np.argmax(activation)] = 1\n",
    "    return loss, one_hot_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(x, y):\n",
    "    loss_arr = np.empty(x.shape[0])\n",
    "    one_hot_predictions = np.empty((x.shape[0], output_size))\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        loss_arr[i], one_hot_predictions[i] = feed_forward_sample(x[i], y[i])\n",
    "\n",
    "    print(\"Average loss=\", np.round(np.average(loss_arr), decimals=2))\n",
    "\n",
    "    y_one_hot = np.zeros((y.size, output_size))\n",
    "    y_one_hot[np.arange(y.size), y] = 1\n",
    "\n",
    "    correct_predictions = np.sum(y_one_hot * one_hot_predictions)\n",
    "    correct_pred_percent = format((correct_predictions / y.shape[0]) * 100, \".2f\")\n",
    "    print(\"Accuracy (% of correct predictions):\", correct_predictions, \"/\", y.shape[0], \"[\", correct_pred_percent, \"%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_sample(x, y, lr_rate=0.001, l1=0.01, l2=0.02, momentum=0.9):\n",
    "    sample = x.flatten()\n",
    "    activations = calc_activation(sample, weights)\n",
    "    a = activations[-1]\n",
    "    one_hot_y = one_hot_encoder(y, output_size)\n",
    "    # loss = cross_entropy_loss(one_hot_y, sample)\n",
    "    # # apply penalties\n",
    "    # for w, b in zip(weights, biases):\n",
    "    #     loss += l1 * (np.sum(abs(w)) + np.sum(abs(b))) + l2 * (np.sum((w) ** 2) + np.sum((b) ** 2))\n",
    "\n",
    "    one_hot_guess = np.zeros_like(a)\n",
    "    one_hot_guess[np.argmax(a)] = 1\n",
    "\n",
    "    weight_gradients = [None] * len(weights)\n",
    "    bias_gradients = [None] * len(weights)\n",
    "    activation_gradients = [None] * (len(weights) - 1)\n",
    "    # change_w = np.zeros(len(weights))\n",
    "    # change_b = np.zeros(len(biases))\n",
    "\n",
    "    for i in reversed(range(len(weights))):\n",
    "\n",
    "        if i == len(weights) - 1:\n",
    "            y = one_hot_y.reshape(-1, 1)\n",
    "            a = activations[i].reshape(-1, 1)\n",
    "            a_prev = activations[i - 1].reshape(-1, 1)\n",
    "\n",
    "            weight_gradients[i] = np.dot((a - y), a_prev.T)\n",
    "            bias_gradients[i] = a - y\n",
    "        else:\n",
    "            w_next = weights[i + 1]\n",
    "            a_next = activations[i + 1].reshape(-1, 1)\n",
    "            y = one_hot_y.reshape(-1, 1)\n",
    "            a = activations[i].reshape(-1, 1)\n",
    "            if i > 0:\n",
    "                a_prev = activations[i - 1].reshape(-1, 1)\n",
    "            else:\n",
    "                a_prev = x.flatten().reshape(-1, 1)\n",
    "\n",
    "            if i == len(weights) - 2:\n",
    "                dA = np.dot(w_next.T, (a_next - y))\n",
    "                activation_gradients[i] = dA\n",
    "            else:\n",
    "                dA_next = activation_gradients[i + 1]\n",
    "                dA = np.dot(w_next.T, (der_sigmoid(a_next) * dA_next))\n",
    "                activation_gradients[i] = dA\n",
    "\n",
    "            z = der_sigmoid(a) * dA\n",
    "            weight_gradients[i] = np.dot(z, a_prev.T)\n",
    "            bias_gradients[i] = z\n",
    "\n",
    "        #implementing Nesterov Momentum\n",
    "        # change_w[i] = -weight_gradients[i] * lr_rate + (momentum * change_w[i])\n",
    "        # change_w[i] = -weight_gradients[i] * lr_rate + (momentum * change_w[i])\n",
    "        weights[i] -= weight_gradients[i] * lr_rate\n",
    "        biases[i] -= bias_gradients[i].flatten() * lr_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dataset(learning_rate=0.0001):\n",
    "    for idx in range(x_train.shape[0]):\n",
    "        train_one_sample(x_train[idx], y_train[idx], learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_epoch():\n",
    "    train_dataset()\n",
    "    evaluate_dataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 5\n",
      "Average loss= 1360.59\n",
      "Accuracy (% of correct predictions): 8371.0 / 10000 [ 83.71 %]\n",
      "\n",
      "Epoch 2 / 5\n",
      "Average loss= 2311.15\n",
      "Accuracy (% of correct predictions): 8520.0 / 10000 [ 85.20 %]\n",
      "\n",
      "Epoch 3 / 5\n",
      "Average loss= 3458.39\n",
      "Accuracy (% of correct predictions): 8627.0 / 10000 [ 86.27 %]\n",
      "\n",
      "Epoch 4 / 5\n",
      "Average loss= 4764.76\n",
      "Accuracy (% of correct predictions): 8736.0 / 10000 [ 87.36 %]\n",
      "\n",
      "Epoch 5 / 5\n",
      "Average loss= 6229.29\n",
      "Accuracy (% of correct predictions): 8715.0 / 10000 [ 87.15 %]\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(\"\\nEpoch {} / {}\".format(e + 1, epochs))\n",
    "    train_test_epoch()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
